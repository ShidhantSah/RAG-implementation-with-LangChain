{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.16\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]= \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-CHATBOT\"\n",
    "os.environ[\"GROQ_API_KEY\"]=\"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\"\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile! Do you want to hear another one?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 16, 'total_tokens': 57, 'completion_time': 0.034166667, 'prompt_time': 0.00300308, 'queue_time': 0.018053675999999998, 'total_time': 0.037169747}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-de5011b2-9b08-4cf1-95c8-c3af28f33740-0', usage_metadata={'input_tokens': 16, 'output_tokens': 41, 'total_tokens': 57})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = model | output_parser\n",
    "result = chain.invoke(\"Tell me a joke\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phone_model='iPhone 14' rating=4.5 pros=['crystal clear and vibrant display', 'smooth performance', 'fast and accurate Face ID', 'amazing camera', 'solid battery life'] cons=[\"design hasn't changed much\", 'expensive', 'no charger in the box'] summary='A great upgrade from my old phone, but with some drawbacks.'\n",
      "['crystal clear and vibrant display', 'smooth performance', 'fast and accurate Face ID', 'amazing camera', 'solid battery life']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MobileReview(BaseModel):\n",
    "        phone_model: str = Field(description=\"Name and model of the phone\")\n",
    "        rating: float = Field(description=\"Overall rating out of 5\")\n",
    "        pros: List[str] = Field(description=\"List of positive aspects\")\n",
    "        cons: List[str] = Field(description=\"List of negative aspects\")\n",
    "        summary: str = Field(description=\"Brief summary of the review\")\n",
    "\n",
    "review_text = \"\"\"\n",
    "    Recently upgraded to the iPhone 14, and I'm loving it so far. The display is crystal clear and vibrant,\n",
    "    definitely an upgrade from my old phone. The new A15 Bionic chip makes everything feel smooth, even heavy apps.\n",
    "    Face ID is faster and more accurate than ever. Plus, the camera takes amazing photos, especially in low light!\n",
    "    The battery life has been solid too, getting me through a full day with ease.\n",
    "    On the downside, the design hasn’t changed much from last year. Also, it’s still pretty expensive, which is a bit of a turnoff.\n",
    "    And yes, no charger in the box, which feels like a missed opportunity.\n",
    "    All in all, I’d rate it a 4.5 out of 5. If you’re into Apple’s ecosystem and can justify the price, it’s definitely worth it.\n",
    "    \"\"\"\n",
    "\n",
    "structured_llm = model.with_structured_output(MobileReview)\n",
    "output = structured_llm.invoke(review_text)\n",
    "print(output)\n",
    "print(output.pros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the programmer quit his job?\n",
      "\n",
      "Because he didn't get arrays! (get a raise)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "chain = prompt | model | output_parser\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Programming! It\\'s like trying to solve a puzzle blindfolded while being chased by a pack of wild code-sniffing dogs!\\n\\nBut seriously, programming is like building with Legos, except instead of making a cool castle, you\\'re creating a complex piece of software that can do amazing things!\\n\\nDid you hear about the programmer who quit his job to pursue his dream of becoming a baker? He didn\\'t have the \"bread\" to continue!\\n\\nOr how about this one: Why do programmers prefer dark mode? Because light attracts bugs!\\n\\nOkay, okay, I know, I know, programming can be tough and frustrating at times. But don\\'t worry, with practice and patience, you\\'ll be coding like a pro in no time!\\n\\nAnd remember, coding is like playing a game – you make mistakes, you learn from them, and you level up!\\n\\nSo, what kind of programming do you want to learn?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 185, 'prompt_tokens': 28, 'total_tokens': 213, 'completion_time': 0.154166667, 'prompt_time': 0.003240377, 'queue_time': 0.022785251, 'total_time': 0.157407044}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-7922cc67-1a8e-4cd4-9076-6b2d637ac374-0' usage_metadata={'input_tokens': 28, 'output_tokens': 185, 'total_tokens': 213}\n",
      "content=\"Programming! It's like trying to solve a puzzle blindfolded while being chased by a pack of wild animals... but in a good way!\\n\\nBut seriously, programming is like building with Legos, but instead of blocks, you're working with code. You write instructions that a computer can understand, and it does its best to follow them. It's like writing a recipe for the computer, and it's up to you to make sure it turns out just right!\\n\\nBut don't worry, it's not all code and no fun! Programming has its own special kind of humor, like:\\n\\nWhy do programmers prefer dark mode? Because light attracts bugs.\\n\\nOr:\\n\\nWhy do programmers love coffee? Because it's the only thing that can debug their lack of sleep.\\n\\nAnd:\\n\\nWhy did the programmer quit his job? Because he didn't get arrays!\\n\\nSo, if you're interested in learning more about programming, just remember: it's like building a Lego castle, one line of code at a time!\\n\\nWould you like to learn more about a specific type of programming, like Python or JavaScript?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 220, 'prompt_tokens': 28, 'total_tokens': 248, 'completion_time': 0.183333333, 'prompt_time': 0.004667272, 'queue_time': 0.023255996, 'total_time': 0.188000605}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-4d10251e-6050-4f9b-a0c0-b399676ea1b7-0' usage_metadata={'input_tokens': 28, 'output_tokens': 220, 'total_tokens': 248}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "        HumanMessage(content=\"Tell me about programming\")\n",
    "    ]\n",
    "response = model.invoke(messages)\n",
    "print(response)\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "        (\"human\", \"Tell me about {topic}\")\n",
    "    ])\n",
    "chain = template | model\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "folder_path = \"/\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
    "print(document_embeddings[0][:5]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "collection_name = \"Essays\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was GreenGrow Innovations founded?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "retriever_results = retriever.invoke(\"When was GreenGrow Innovations founded?\")\n",
    "print(retriever_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When was GreenGrow Innovations founded?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = contextualize_q_prompt | model | StrOutputParser()\n",
    "print(contextualize_chain.invoke({\"input\": \"Where is it headquartered?\", \"chat_history\": []}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"When was GreenGrow Innovations founded?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"Where is it headquartered?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNetworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
